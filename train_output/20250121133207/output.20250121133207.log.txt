2025-01-21 13:32:13-finetune.py:240-INFO >> Batch 2 of epoch 1/10, average training loss of previous 2 batches: 1.9327451586723328
2025-01-21 13:32:15-finetune.py:240-INFO >> Batch 4 of epoch 1/10, average training loss of previous 2 batches: 1.5324483513832092
2025-01-21 13:32:18-finetune.py:240-INFO >> Batch 6 of epoch 1/10, average training loss of previous 2 batches: 0.7551045715808868
2025-01-21 13:32:20-finetune.py:240-INFO >> Batch 8 of epoch 1/10, average training loss of previous 2 batches: 1.9983237981796265
2025-01-21 13:32:23-finetune.py:240-INFO >> Batch 10 of epoch 1/10, average training loss of previous 2 batches: 1.4440772533416748
2025-01-21 13:32:25-finetune.py:240-INFO >> Batch 12 of epoch 1/10, average training loss of previous 2 batches: 1.5219286680221558
2025-01-21 13:32:29-finetune.py:240-INFO >> Batch 14 of epoch 1/10, average training loss of previous 2 batches: 1.6459169387817383
2025-01-21 13:32:31-finetune.py:240-INFO >> Batch 16 of epoch 1/10, average training loss of previous 2 batches: 1.3168608248233795
2025-01-21 13:32:33-finetune.py:240-INFO >> Batch 18 of epoch 1/10, average training loss of previous 2 batches: 1.5661260485649109
2025-01-21 13:32:35-finetune.py:240-INFO >> Batch 20 of epoch 1/10, average training loss of previous 2 batches: 1.2978453636169434
2025-01-21 13:32:38-finetune.py:240-INFO >> Batch 22 of epoch 1/10, average training loss of previous 2 batches: 1.2133749127388
2025-01-21 13:32:40-finetune.py:240-INFO >> Batch 24 of epoch 1/10, average training loss of previous 2 batches: 1.3170217871665955
2025-01-21 13:32:42-finetune.py:240-INFO >> Batch 26 of epoch 1/10, average training loss of previous 2 batches: 1.3176246881484985
2025-01-21 13:32:45-finetune.py:240-INFO >> Batch 28 of epoch 1/10, average training loss of previous 2 batches: 1.4187703132629395
2025-01-21 13:32:47-finetune.py:240-INFO >> Batch 30 of epoch 1/10, average training loss of previous 2 batches: 1.435128092765808
2025-01-21 13:32:49-finetune.py:240-INFO >> Batch 32 of epoch 1/10, average training loss of previous 2 batches: 1.4789565205574036
2025-01-21 13:32:51-finetune.py:240-INFO >> Batch 34 of epoch 1/10, average training loss of previous 2 batches: 1.705281376838684
2025-01-21 13:32:54-finetune.py:240-INFO >> Batch 36 of epoch 1/10, average training loss of previous 2 batches: 1.4444853067398071
2025-01-21 13:32:56-finetune.py:240-INFO >> Batch 38 of epoch 1/10, average training loss of previous 2 batches: 1.5444457530975342
2025-01-21 13:32:58-finetune.py:240-INFO >> Batch 40 of epoch 1/10, average training loss of previous 2 batches: 1.116440623998642
2025-01-21 13:33:00-finetune.py:240-INFO >> Batch 42 of epoch 1/10, average training loss of previous 2 batches: 1.250782310962677
2025-01-21 13:33:03-finetune.py:240-INFO >> Batch 44 of epoch 1/10, average training loss of previous 2 batches: 1.2053943872451782
2025-01-21 13:33:06-finetune.py:240-INFO >> Batch 46 of epoch 1/10, average training loss of previous 2 batches: 1.287851333618164
2025-01-21 13:33:08-finetune.py:240-INFO >> Batch 48 of epoch 1/10, average training loss of previous 2 batches: 1.2920907139778137
2025-01-21 13:33:11-finetune.py:240-INFO >> Batch 50 of epoch 1/10, average training loss of previous 2 batches: 1.2608372569084167
2025-01-21 13:33:15-finetune.py:240-INFO >> Batch 52 of epoch 1/10, average training loss of previous 2 batches: 0.9992534518241882
2025-01-21 13:33:20-finetune.py:240-INFO >> Batch 54 of epoch 1/10, average training loss of previous 2 batches: 0.7842737436294556
2025-01-21 13:33:23-finetune.py:240-INFO >> Batch 56 of epoch 1/10, average training loss of previous 2 batches: 1.1116322576999664
2025-01-21 13:33:27-finetune.py:240-INFO >> Batch 58 of epoch 1/10, average training loss of previous 2 batches: 1.084105670452118

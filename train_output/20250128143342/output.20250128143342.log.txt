2025-01-28 14:33:48-finetune_distributed.py:186-INFO >> Batch 1 of epoch 1/3, training loss : 2.142460346221924
2025-01-28 14:33:49-finetune_distributed.py:186-INFO >> Batch 2 of epoch 1/3, training loss : 1.7230299711227417
2025-01-28 14:33:50-finetune_distributed.py:186-INFO >> Batch 3 of epoch 1/3, training loss : 1.369493007659912
2025-01-28 14:33:52-finetune_distributed.py:186-INFO >> Batch 4 of epoch 1/3, training loss : 1.6960560083389282
2025-01-28 14:33:53-finetune_distributed.py:186-INFO >> Batch 5 of epoch 1/3, training loss : 0.8216977715492249
2025-01-28 14:33:54-finetune_distributed.py:186-INFO >> Batch 6 of epoch 1/3, training loss : 0.7217775583267212
2025-01-28 14:33:56-finetune_distributed.py:186-INFO >> Batch 7 of epoch 1/3, training loss : 2.004068374633789
2025-01-28 14:33:57-finetune_distributed.py:186-INFO >> Batch 8 of epoch 1/3, training loss : 2.020756959915161
2025-01-28 14:33:59-finetune_distributed.py:186-INFO >> Batch 9 of epoch 1/3, training loss : 1.6900718212127686
2025-01-28 14:34:01-finetune_distributed.py:186-INFO >> Batch 10 of epoch 1/3, training loss : 1.2414770126342773
2025-01-28 14:34:02-finetune_distributed.py:186-INFO >> Batch 11 of epoch 1/3, training loss : 1.1497889757156372
2025-01-28 14:34:03-finetune_distributed.py:186-INFO >> Batch 12 of epoch 1/3, training loss : 1.9305047988891602
2025-01-28 14:34:05-finetune_distributed.py:186-INFO >> Batch 13 of epoch 1/3, training loss : 1.7417232990264893
2025-01-28 14:34:07-finetune_distributed.py:186-INFO >> Batch 14 of epoch 1/3, training loss : 1.587354063987732
2025-01-28 14:34:08-finetune_distributed.py:186-INFO >> Batch 15 of epoch 1/3, training loss : 1.8601104021072388
2025-01-28 14:34:09-finetune_distributed.py:186-INFO >> Batch 16 of epoch 1/3, training loss : 0.8015508055686951
2025-01-28 14:34:10-finetune_distributed.py:186-INFO >> Batch 17 of epoch 1/3, training loss : 1.7362825870513916
2025-01-28 14:34:12-finetune_distributed.py:186-INFO >> Batch 18 of epoch 1/3, training loss : 1.43684720993042
2025-01-28 14:34:13-finetune_distributed.py:186-INFO >> Batch 19 of epoch 1/3, training loss : 1.4593628644943237
2025-01-28 14:34:14-finetune_distributed.py:186-INFO >> Batch 20 of epoch 1/3, training loss : 1.1524648666381836
2025-01-28 14:34:15-finetune_distributed.py:186-INFO >> Batch 21 of epoch 1/3, training loss : 1.0915364027023315
2025-01-28 14:34:17-finetune_distributed.py:186-INFO >> Batch 22 of epoch 1/3, training loss : 1.3551405668258667
2025-01-28 14:34:18-finetune_distributed.py:186-INFO >> Batch 23 of epoch 1/3, training loss : 1.3342859745025635
2025-01-28 14:34:19-finetune_distributed.py:186-INFO >> Batch 24 of epoch 1/3, training loss : 1.3250242471694946
2025-01-28 14:34:21-finetune_distributed.py:186-INFO >> Batch 25 of epoch 1/3, training loss : 1.4146246910095215
2025-01-28 14:34:22-finetune_distributed.py:186-INFO >> Batch 26 of epoch 1/3, training loss : 1.2614502906799316
2025-01-28 14:34:23-finetune_distributed.py:186-INFO >> Batch 27 of epoch 1/3, training loss : 1.5974570512771606
2025-01-28 14:34:25-finetune_distributed.py:186-INFO >> Batch 28 of epoch 1/3, training loss : 1.2625975608825684
2025-01-28 14:34:26-finetune_distributed.py:186-INFO >> Batch 29 of epoch 1/3, training loss : 1.5652803182601929
2025-01-28 14:34:27-finetune_distributed.py:186-INFO >> Batch 30 of epoch 1/3, training loss : 1.3309974670410156
2025-01-28 14:34:28-finetune_distributed.py:186-INFO >> Batch 31 of epoch 1/3, training loss : 1.6966464519500732
2025-01-28 14:34:30-finetune_distributed.py:186-INFO >> Batch 32 of epoch 1/3, training loss : 1.2998085021972656
2025-01-28 14:34:31-finetune_distributed.py:186-INFO >> Batch 33 of epoch 1/3, training loss : 1.8623971939086914
2025-01-28 14:34:32-finetune_distributed.py:186-INFO >> Batch 34 of epoch 1/3, training loss : 1.6041029691696167
2025-01-28 14:34:33-finetune_distributed.py:186-INFO >> Batch 35 of epoch 1/3, training loss : 1.6317232847213745
2025-01-28 14:34:35-finetune_distributed.py:186-INFO >> Batch 36 of epoch 1/3, training loss : 1.3089568614959717
2025-01-28 14:34:36-finetune_distributed.py:186-INFO >> Batch 37 of epoch 1/3, training loss : 1.611844778060913

2025-01-28 13:08:41-finetune_distributed.py:184-INFO >> Batch 1 of epoch 1/10, training loss : 2.0923731327056885
2025-01-28 13:08:42-finetune_distributed.py:184-INFO >> Batch 2 of epoch 1/10, training loss : 2.1742494106292725
2025-01-28 13:08:44-finetune_distributed.py:184-INFO >> Batch 3 of epoch 1/10, training loss : 2.0273849964141846
2025-01-28 13:08:45-finetune_distributed.py:184-INFO >> Batch 4 of epoch 1/10, training loss : 1.3198503255844116
2025-01-28 13:08:47-finetune_distributed.py:184-INFO >> Batch 5 of epoch 1/10, training loss : 1.3410433530807495
2025-01-28 13:08:49-finetune_distributed.py:184-INFO >> Batch 6 of epoch 1/10, training loss : 1.3907357454299927
2025-01-28 13:08:50-finetune_distributed.py:184-INFO >> Batch 7 of epoch 1/10, training loss : 1.6106836795806885
2025-01-28 13:08:52-finetune_distributed.py:184-INFO >> Batch 8 of epoch 1/10, training loss : 1.5927058458328247
2025-01-28 13:08:54-finetune_distributed.py:184-INFO >> Batch 9 of epoch 1/10, training loss : 0.7498422265052795
2025-01-28 13:08:55-finetune_distributed.py:184-INFO >> Batch 10 of epoch 1/10, training loss : 0.8106973171234131
2025-01-28 13:08:57-finetune_distributed.py:184-INFO >> Batch 11 of epoch 1/10, training loss : 0.7418087124824524
2025-01-28 13:08:58-finetune_distributed.py:184-INFO >> Batch 12 of epoch 1/10, training loss : 0.643606424331665
2025-01-28 13:09:00-finetune_distributed.py:184-INFO >> Batch 13 of epoch 1/10, training loss : 2.008526563644409
2025-01-28 13:09:02-finetune_distributed.py:184-INFO >> Batch 14 of epoch 1/10, training loss : 1.932835578918457
2025-01-28 13:09:04-finetune_distributed.py:184-INFO >> Batch 15 of epoch 1/10, training loss : 2.0452892780303955
2025-01-28 13:09:05-finetune_distributed.py:184-INFO >> Batch 16 of epoch 1/10, training loss : 1.8793741464614868
2025-01-28 13:09:07-finetune_distributed.py:184-INFO >> Batch 17 of epoch 1/10, training loss : 1.5101935863494873
2025-01-28 13:09:09-finetune_distributed.py:184-INFO >> Batch 18 of epoch 1/10, training loss : 1.8249419927597046
2025-01-28 13:09:10-finetune_distributed.py:184-INFO >> Batch 19 of epoch 1/10, training loss : 1.8659976720809937
2025-01-28 13:09:12-finetune_distributed.py:184-INFO >> Batch 20 of epoch 1/10, training loss : 0.6263055205345154
2025-01-28 13:09:14-finetune_distributed.py:184-INFO >> Batch 21 of epoch 1/10, training loss : 1.6867337226867676
2025-01-28 13:09:15-finetune_distributed.py:184-INFO >> Batch 22 of epoch 1/10, training loss : 0.7269993424415588
2025-01-28 13:09:17-finetune_distributed.py:184-INFO >> Batch 23 of epoch 1/10, training loss : 2.1699695587158203
2025-01-28 13:09:18-finetune_distributed.py:184-INFO >> Batch 24 of epoch 1/10, training loss : 1.6378841400146484
2025-01-28 13:09:20-finetune_distributed.py:184-INFO >> Batch 25 of epoch 1/10, training loss : 1.813126564025879
2025-01-28 13:09:22-finetune_distributed.py:184-INFO >> Batch 26 of epoch 1/10, training loss : 1.6161915063858032
2025-01-28 13:09:23-finetune_distributed.py:184-INFO >> Batch 27 of epoch 1/10, training loss : 1.3483620882034302
2025-01-28 13:09:25-finetune_distributed.py:184-INFO >> Batch 28 of epoch 1/10, training loss : 1.8097329139709473
2025-01-28 13:09:27-finetune_distributed.py:184-INFO >> Batch 29 of epoch 1/10, training loss : 1.961190938949585
2025-01-28 13:09:29-finetune_distributed.py:184-INFO >> Batch 30 of epoch 1/10, training loss : 1.7141269445419312
2025-01-28 13:09:30-finetune_distributed.py:184-INFO >> Batch 31 of epoch 1/10, training loss : 0.282370001077652
2025-01-28 13:09:32-finetune_distributed.py:184-INFO >> Batch 32 of epoch 1/10, training loss : 1.4694031476974487
2025-01-28 13:09:33-finetune_distributed.py:184-INFO >> Batch 33 of epoch 1/10, training loss : 1.679834008216858
2025-01-28 13:09:35-finetune_distributed.py:184-INFO >> Batch 34 of epoch 1/10, training loss : 1.7324625253677368
2025-01-28 13:09:37-finetune_distributed.py:184-INFO >> Batch 35 of epoch 1/10, training loss : 1.4210631847381592
2025-01-28 13:09:38-finetune_distributed.py:184-INFO >> Batch 36 of epoch 1/10, training loss : 1.4099173545837402
2025-01-28 13:09:40-finetune_distributed.py:184-INFO >> Batch 37 of epoch 1/10, training loss : 1.4676600694656372
2025-01-28 13:09:41-finetune_distributed.py:184-INFO >> Batch 38 of epoch 1/10, training loss : 1.3995530605316162
2025-01-28 13:09:43-finetune_distributed.py:184-INFO >> Batch 39 of epoch 1/10, training loss : 1.3377110958099365
2025-01-28 13:09:45-finetune_distributed.py:184-INFO >> Batch 40 of epoch 1/10, training loss : 1.0117173194885254
2025-01-28 13:09:46-finetune_distributed.py:184-INFO >> Batch 41 of epoch 1/10, training loss : 0.6951645612716675
2025-01-28 13:09:48-finetune_distributed.py:184-INFO >> Batch 42 of epoch 1/10, training loss : 1.4851394891738892
2025-01-28 13:09:50-finetune_distributed.py:184-INFO >> Batch 43 of epoch 1/10, training loss : 1.3562570810317993
2025-01-28 13:09:51-finetune_distributed.py:184-INFO >> Batch 44 of epoch 1/10, training loss : 1.2903202772140503
2025-01-28 13:09:53-finetune_distributed.py:184-INFO >> Batch 45 of epoch 1/10, training loss : 1.5111759901046753
2025-01-28 13:09:55-finetune_distributed.py:184-INFO >> Batch 46 of epoch 1/10, training loss : 1.1129944324493408
2025-01-28 13:09:56-finetune_distributed.py:184-INFO >> Batch 47 of epoch 1/10, training loss : 1.148630142211914
2025-01-28 13:09:58-finetune_distributed.py:184-INFO >> Batch 48 of epoch 1/10, training loss : 1.4457658529281616
2025-01-28 13:09:59-finetune_distributed.py:184-INFO >> Batch 49 of epoch 1/10, training loss : 1.1840167045593262
2025-01-28 13:10:01-finetune_distributed.py:184-INFO >> Batch 50 of epoch 1/10, training loss : 1.5771514177322388
2025-01-28 13:10:03-finetune_distributed.py:184-INFO >> Batch 51 of epoch 1/10, training loss : 1.5592588186264038
2025-01-28 13:10:04-finetune_distributed.py:184-INFO >> Batch 52 of epoch 1/10, training loss : 0.9633263349533081
2025-01-28 13:10:06-finetune_distributed.py:184-INFO >> Batch 53 of epoch 1/10, training loss : 1.8164154291152954
2025-01-28 13:10:08-finetune_distributed.py:184-INFO >> Batch 54 of epoch 1/10, training loss : 1.3526034355163574
2025-01-28 13:10:09-finetune_distributed.py:184-INFO >> Batch 55 of epoch 1/10, training loss : 1.1097348928451538
2025-01-28 13:10:11-finetune_distributed.py:184-INFO >> Batch 56 of epoch 1/10, training loss : 1.330019235610962
2025-01-28 13:10:13-finetune_distributed.py:184-INFO >> Batch 57 of epoch 1/10, training loss : 1.430498480796814
2025-01-28 13:10:14-finetune_distributed.py:184-INFO >> Batch 58 of epoch 1/10, training loss : 1.6178176403045654
2025-01-28 13:10:16-finetune_distributed.py:184-INFO >> Batch 59 of epoch 1/10, training loss : 1.2172218561172485
2025-01-28 13:10:17-finetune_distributed.py:184-INFO >> Batch 60 of epoch 1/10, training loss : 1.4297739267349243
2025-01-28 13:10:19-finetune_distributed.py:184-INFO >> Batch 61 of epoch 1/10, training loss : 1.550591230392456
2025-01-28 13:10:21-finetune_distributed.py:184-INFO >> Batch 62 of epoch 1/10, training loss : 1.7449119091033936
2025-01-28 13:10:22-finetune_distributed.py:184-INFO >> Batch 63 of epoch 1/10, training loss : 1.671555519104004
2025-01-28 13:10:24-finetune_distributed.py:184-INFO >> Batch 64 of epoch 1/10, training loss : 0.9236040711402893
2025-01-28 13:10:26-finetune_distributed.py:184-INFO >> Batch 65 of epoch 1/10, training loss : 2.0274007320404053
2025-01-28 13:10:27-finetune_distributed.py:184-INFO >> Batch 66 of epoch 1/10, training loss : 1.6533598899841309
2025-01-28 13:10:29-finetune_distributed.py:184-INFO >> Batch 67 of epoch 1/10, training loss : 1.4083398580551147
2025-01-28 13:10:30-finetune_distributed.py:184-INFO >> Batch 68 of epoch 1/10, training loss : 1.7099639177322388
2025-01-28 13:10:32-finetune_distributed.py:184-INFO >> Batch 69 of epoch 1/10, training loss : 1.5005238056182861
2025-01-28 13:10:34-finetune_distributed.py:184-INFO >> Batch 70 of epoch 1/10, training loss : 1.683518648147583
2025-01-28 13:10:35-finetune_distributed.py:184-INFO >> Batch 71 of epoch 1/10, training loss : 0.9971048831939697
2025-01-28 13:10:37-finetune_distributed.py:184-INFO >> Batch 72 of epoch 1/10, training loss : 1.804961919784546
2025-01-28 13:10:38-finetune_distributed.py:184-INFO >> Batch 73 of epoch 1/10, training loss : 1.925132393836975
2025-01-28 13:10:40-finetune_distributed.py:184-INFO >> Batch 74 of epoch 1/10, training loss : 1.3202894926071167
2025-01-28 13:10:41-finetune_distributed.py:184-INFO >> Batch 75 of epoch 1/10, training loss : 1.6384905576705933
2025-01-28 13:10:43-finetune_distributed.py:184-INFO >> Batch 76 of epoch 1/10, training loss : 1.3172551393508911
2025-01-28 13:10:45-finetune_distributed.py:184-INFO >> Batch 77 of epoch 1/10, training loss : 0.6383338570594788
2025-01-28 13:10:46-finetune_distributed.py:184-INFO >> Batch 78 of epoch 1/10, training loss : 1.5322065353393555
2025-01-28 13:10:48-finetune_distributed.py:184-INFO >> Batch 79 of epoch 1/10, training loss : 1.6401087045669556
2025-01-28 13:10:49-finetune_distributed.py:184-INFO >> Batch 80 of epoch 1/10, training loss : 0.9844351410865784
2025-01-28 13:10:51-finetune_distributed.py:184-INFO >> Batch 81 of epoch 1/10, training loss : 0.718040943145752
2025-01-28 13:10:53-finetune_distributed.py:184-INFO >> Batch 82 of epoch 1/10, training loss : 1.5060347318649292
2025-01-28 13:10:54-finetune_distributed.py:184-INFO >> Batch 83 of epoch 1/10, training loss : 1.4052401781082153
2025-01-28 13:10:56-finetune_distributed.py:184-INFO >> Batch 84 of epoch 1/10, training loss : 1.311340093612671
2025-01-28 13:10:58-finetune_distributed.py:184-INFO >> Batch 85 of epoch 1/10, training loss : 1.2224047183990479
2025-01-28 13:10:59-finetune_distributed.py:184-INFO >> Batch 86 of epoch 1/10, training loss : 1.2081040143966675
2025-01-28 13:11:01-finetune_distributed.py:184-INFO >> Batch 87 of epoch 1/10, training loss : 1.2503100633621216
2025-01-28 13:11:03-finetune_distributed.py:184-INFO >> Batch 88 of epoch 1/10, training loss : 1.1270802021026611
2025-01-28 13:11:05-finetune_distributed.py:184-INFO >> Batch 89 of epoch 1/10, training loss : 1.532814860343933
2025-01-28 13:11:06-finetune_distributed.py:184-INFO >> Batch 90 of epoch 1/10, training loss : 1.2212525606155396

2025-01-28 15:09:25-finetune_distributed.py:193-INFO >> Batch 1 of epoch 1/3, training loss : 2.091158390045166
2025-01-28 15:09:26-finetune_distributed.py:193-INFO >> Batch 2 of epoch 1/3, training loss : 1.438332200050354
2025-01-28 15:09:27-finetune_distributed.py:193-INFO >> Batch 3 of epoch 1/3, training loss : 0.8006381988525391
2025-01-28 15:09:29-finetune_distributed.py:193-INFO >> Batch 4 of epoch 1/3, training loss : 2.114680051803589
2025-01-28 15:09:30-finetune_distributed.py:193-INFO >> Batch 5 of epoch 1/3, training loss : 1.5739635229110718
2025-01-28 15:09:31-finetune_distributed.py:193-INFO >> Batch 6 of epoch 1/3, training loss : 1.7777496576309204
2025-01-28 15:09:32-finetune_distributed.py:193-INFO >> Batch 7 of epoch 1/3, training loss : 1.8770586252212524
2025-01-28 15:09:34-finetune_distributed.py:193-INFO >> Batch 8 of epoch 1/3, training loss : 2.0366392135620117
2025-01-28 15:09:34-finetune_distributed.py:193-INFO >> Batch 9 of epoch 1/3, training loss : 1.7337992191314697
2025-01-28 15:09:36-finetune_distributed.py:193-INFO >> Batch 10 of epoch 1/3, training loss : 1.5874205827713013
2025-01-28 15:09:37-finetune_distributed.py:193-INFO >> Batch 11 of epoch 1/3, training loss : 0.7369377017021179
2025-01-28 15:09:38-finetune_distributed.py:193-INFO >> Batch 12 of epoch 1/3, training loss : 1.553323745727539
2025-01-28 15:09:39-finetune_distributed.py:193-INFO >> Batch 13 of epoch 1/3, training loss : 1.2576642036437988
2025-01-28 15:09:41-finetune_distributed.py:193-INFO >> Batch 14 of epoch 1/3, training loss : 1.847280502319336
2025-01-28 15:09:42-finetune_distributed.py:193-INFO >> Batch 15 of epoch 1/3, training loss : 1.4717572927474976
2025-01-28 15:09:43-finetune_distributed.py:193-INFO >> Batch 16 of epoch 1/3, training loss : 1.6620903015136719
2025-01-28 15:09:44-finetune_distributed.py:193-INFO >> Batch 17 of epoch 1/3, training loss : 2.030810594558716
2025-01-28 15:09:46-finetune_distributed.py:193-INFO >> Batch 18 of epoch 1/3, training loss : 1.5615993738174438
2025-01-28 15:09:46-finetune_distributed.py:193-INFO >> Batch 19 of epoch 1/3, training loss : 1.992790937423706
2025-01-28 15:09:48-finetune_distributed.py:193-INFO >> Batch 20 of epoch 1/3, training loss : 0.6717809438705444
2025-01-28 15:09:49-finetune_distributed.py:193-INFO >> Batch 21 of epoch 1/3, training loss : 0.7477171421051025
2025-01-28 15:09:50-finetune_distributed.py:193-INFO >> Batch 22 of epoch 1/3, training loss : 1.273951530456543
2025-01-28 15:09:51-finetune_distributed.py:193-INFO >> Batch 23 of epoch 1/3, training loss : 1.576759934425354
2025-01-28 15:09:53-finetune_distributed.py:193-INFO >> Batch 24 of epoch 1/3, training loss : 1.1445776224136353
2025-01-28 15:09:54-finetune_distributed.py:193-INFO >> Batch 25 of epoch 1/3, training loss : 1.6391308307647705
2025-01-28 15:09:56-finetune_distributed.py:193-INFO >> Batch 26 of epoch 1/3, training loss : 1.3438794612884521
2025-01-28 15:09:57-finetune_distributed.py:193-INFO >> Batch 27 of epoch 1/3, training loss : 0.9008071422576904
2025-01-28 15:09:59-finetune_distributed.py:193-INFO >> Batch 28 of epoch 1/3, training loss : 1.399754285812378
2025-01-28 15:10:00-finetune_distributed.py:193-INFO >> Batch 29 of epoch 1/3, training loss : 1.027413010597229
2025-01-28 15:10:03-finetune_distributed.py:193-INFO >> Batch 30 of epoch 1/3, training loss : 0.9403313398361206
2025-01-28 15:10:04-finetune_distributed.py:193-INFO >> Batch 31 of epoch 1/3, training loss : 1.1369941234588623
2025-01-28 15:10:06-finetune_distributed.py:193-INFO >> Batch 32 of epoch 1/3, training loss : 1.2596737146377563
